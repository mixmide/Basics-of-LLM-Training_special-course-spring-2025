{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Домашнее задание №4: fine-tuning**"
      ],
      "metadata": {
        "id": "adokFBz1yeR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Цель задания\n",
        "\n",
        "Наша задача заключается в выполнении fine-tuning для модели с использованием технологии LoRa на основе данных из ДЗ-2: так, будем использовать модель **RuadaptQwen2.5-1.5B-instruct**, показавшую более высокие результаты, и датасет **ai-forever/MERA** (сабсет rcb). Сравним качество модели до и после дообучения на тестовой выборке (т.е. split *'validation'* из рассматриваемого датасета)."
      ],
      "metadata": {
        "id": "2c6XbzipB-xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим содержимое датасета *ai-forever/MERA* (сабсет *rcb*):\n",
        "*   instruction: содержит задачу-инструкцию (анализ гипотезы на основе некоторого контекста),\n",
        "*   inputs: тексты с контекстом и гипотезой для поля instructions,\n",
        "*   outputs: метка в виде цифры (1, 2 или 3), означающая правильный ответ,\n",
        "*   meta: доп. информация.\n",
        "\n",
        "Поля 'instruction' и 'inputs' объединяем в единый промпт, после чего модель должна сгенерировать цифру-ответ, и, если ответ правильный, он должен совпасть с соответствующей цифрой в поле 'outputs'."
      ],
      "metadata": {
        "id": "X8PQUSiVCAHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#План работы\n",
        "\n",
        "Для текущего задания нам потребуется:\n",
        "*   Разбить данные на train и test;\n",
        "*   Взять и подготовить модель RuadaptQwen2.5-1.5B-instruct;\n",
        "*   Проанализировать качество исходной модели (дополнительно рассмотрим поведение на few-shot промпте);\n",
        "*   Провести fine-tuning с использованием LoRa, используя split 'train';\n",
        "*   Сравнить качество (accuracy) до и после fine-tuning, сделать выводы."
      ],
      "metadata": {
        "id": "JHs4KafNCATW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Подготовка\n",
        "\n",
        "Начинаем с импортов и загрузки:"
      ],
      "metadata": {
        "id": "UeWAN6IUCAbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJH28vV4x2kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7779f174-2d9b-4139-9ec6-2c83dcdd887a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate       # для ускорения обучения\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForTokenClassification, GenerationConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ai-forever/MERA\", \"rcb\")\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"validation\"]\n",
        "\n",
        "print(f\"\\nРазмер Train: {len(train_dataset)}, Размер Test (точнее, split 'Validation'): {len(test_dataset)}\")\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i}: outputs = '{test_dataset[i]['outputs']}'\")"
      ],
      "metadata": {
        "id": "TYynWWnn5WRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3af360-46ef-4176-f70e-421b856851e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Размер Train: 438, Размер Test (точнее, split 'Validation'): 220\n",
            "Sample 0: outputs = '3'\n",
            "Sample 1: outputs = '2'\n",
            "Sample 2: outputs = '2'\n",
            "Sample 3: outputs = '3'\n",
            "Sample 4: outputs = '1'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию *create_messages*, которая формирует формат сообщений:\n",
        "\n",
        "* system для роли модели;\n",
        "* user для роли пользователя;\n",
        "* assistant для хранения правильного ответа, используется при обучении.\n",
        "\n",
        "Для каждой записи в обучающем и тестовом датасете будем формировать список сообщений в нужном формате:"
      ],
      "metadata": {
        "id": "7DuruuXpK4Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_messages(sample):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"Вы - самый умный и логичный в мире помощник. Пользователь будет предоставлять, во-первых, текст с описанием ситуации, во-вторых, гипотезу. Тебе нужно понять, есть ли взаимосвязь гипотезы с описанной ситуацией, и выбрать один из трех вариантов: 1 - гипотеза следует из ситуации, 2 - гипотеза противоречит ситуации, 3 - гипотеза независима от ситуации.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{sample['instruction']}\\n{sample['inputs']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": str(sample['outputs'])}\n",
        "    ]\n",
        "\n",
        "train_messages = [create_messages(sample) for sample in train_dataset]\n",
        "test_messages = [create_messages(sample) for sample in test_dataset]"
      ],
      "metadata": {
        "id": "v1-SDx1p5WTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим нашу модель и соответствующий ей токенизатор:"
      ],
      "metadata": {
        "id": "iK5adQVgS5Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"RefalMachine/RuadaptQwen2.5-1.5B-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"sdpa\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y-Gt_IgF5WW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее мы создадим класс ChatDataset для подготовки данных к обучению.\n",
        "\n",
        "* метод **__init__**: для каждой записи с помощью метода convert_record создаются тензоры с входными данными (input_ids), метками (labels) и attention_mask;\n",
        "\n",
        "* метод **convert_record** обрабатывает каждое сообщение из диалога, преобразуя его в последовательность id токенов с помощью apply_chat_template токенизатора.\n",
        "\n",
        "Полученные тензоры затем используются для формирования train_chat_dataset и test_chat_dataset."
      ],
      "metadata": {
        "id": "E53qkFQ9TxJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_records: List[Dict],\n",
        "        tokenizer: AutoTokenizer,\n",
        "        max_tokens_count: int,\n",
        "        only_target_loss: bool = True,\n",
        "        labels_pad_token_id: int = -100,\n",
        "    ):\n",
        "        self.original_records = original_records\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_tokens_count = max_tokens_count\n",
        "        self.only_target_loss = only_target_loss\n",
        "        self.labels_pad_token_id = labels_pad_token_id\n",
        "        self.records = []\n",
        "        for record in tqdm(original_records):\n",
        "            tensors = self.convert_record(record)\n",
        "            if tensors is None:\n",
        "                continue\n",
        "            self.records.append(tensors)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.records[index]\n",
        "\n",
        "    def convert_record(self, record):\n",
        "        input_ids, labels = [], []\n",
        "        for message in record:\n",
        "            message_input_ids = self.tokenizer.apply_chat_template(\n",
        "                [message],\n",
        "                add_special_tokens=False,\n",
        "                tokenize=True,\n",
        "                add_generation_prompt=False,\n",
        "            )\n",
        "            if message_input_ids[0] == self.tokenizer.bos_token_id:\n",
        "                message_input_ids = message_input_ids[1:]\n",
        "            message_labels = message_input_ids.copy()\n",
        "            if len(input_ids) + len(message_input_ids) > self.max_tokens_count - 2:\n",
        "                break\n",
        "            if message[\"role\"] != \"assistant\" and self.only_target_loss:\n",
        "                message_labels = [self.labels_pad_token_id] * len(message_input_ids)\n",
        "            input_ids.extend(message_input_ids)\n",
        "            labels.extend(message_labels)\n",
        "        if not input_ids:\n",
        "            return None\n",
        "        if input_ids[0] != self.tokenizer.bos_token_id:\n",
        "            input_ids.insert(0, self.tokenizer.bos_token_id)\n",
        "            labels.insert(0, self.labels_pad_token_id)\n",
        "        if input_ids[-1] != self.tokenizer.eos_token_id:\n",
        "            input_ids.append(self.tokenizer.eos_token_id)\n",
        "            labels.append(self.tokenizer.eos_token_id)\n",
        "        input_ids = torch.LongTensor(input_ids)\n",
        "        labels = torch.LongTensor(labels)\n",
        "        attention_mask = input_ids.new_ones(input_ids.size())\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "            \"attention_mask\": attention_mask,\n",
        "        }\n",
        "\n",
        "max_tokens_count = 1024\n",
        "train_chat_dataset = ChatDataset(train_messages, tokenizer, max_tokens_count)\n",
        "test_chat_dataset = ChatDataset(test_messages, tokenizer, max_tokens_count)"
      ],
      "metadata": {
        "id": "FxexB2995WnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fd1813-a7de-4433-8597-6af89df67d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 438/438 [00:01<00:00, 342.75it/s]\n",
            "100%|██████████| 220/220 [00:00<00:00, 317.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию *generate*: она принимает сообщения, модель, токенизатор и конфигурацию генерации. Сначала сообщения преобразуются в input_ids с помощью apply_chat_template, затем выполняется генерация. Далее из полученных токенов удаляются входные и результат декодируется в текст:"
      ],
      "metadata": {
        "id": "BmPmoPecVnIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(messages, model, tokenizer, generation_config):\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=False,\n",
        "        add_generation_prompt=True,\n",
        "    ).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    output_ids = output_ids[:, input_ids.shape[1]:]\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output.strip()\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=1,  # ибо генерим только одну цифру\n",
        "    do_sample=True,\n",
        "    temperature=0.01,\n",
        "    top_k=3,\n",
        "    top_p=0.7,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "VKanxgK5VnVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Оценка модели до fine-tuning"
      ],
      "metadata": {
        "id": "dTESoRsrXXfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in test_dataset:\n",
        "    user_message = {\"role\": \"user\", \"content\": f\"{sample['instruction']}\\n{sample['inputs']}\"}\n",
        "    true_answer = str(sample['outputs'])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Вы - самый умный и логичный в мире помощник. Пользователь будет предоставлять, во-первых, текст с описанием ситуации, во-вторых, гипотезу. Тебе нужно понять, есть ли взаимосвязь гипотезы с описанной ситуацией, и выбрать один из трех вариантов: 1 - гипотеза следует из ситуации, 2 - гипотеза противоречит ситуации, 3 - гипотеза независима от ситуации.\"},\n",
        "        user_message\n",
        "    ]\n",
        "\n",
        "    generated_answer = generate(messages, model, tokenizer, generation_config).strip()\n",
        "    print(f\"True: '{true_answer}', Generated: '{generated_answer}'\")\n",
        "\n",
        "    if generated_answer == true_answer:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH6P0dv7XXqN",
        "outputId": "a7d406b6-e087-47ec-c1b5-fcaec2cd7106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "Accuracy: 0.33181818181818185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, пока что качество очень низкое и находится на уровне случайного угадывания."
      ],
      "metadata": {
        "id": "flORZC7dfYuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Оценка модели до fine-tuning + few-shot\n",
        "\n",
        "Рассмотрим поведение модели при добавлении в промпт нескольких примеров, по одному для каждого класса ответа:"
      ],
      "metadata": {
        "id": "hS_yoY2TYKoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# выбираем несколько примеров из train_dataset (первые три - разных классов)\n",
        "few_shot_examples = [\n",
        "    {\"instruction\": train_dataset[0]['instruction'], \"inputs\": train_dataset[0]['inputs'], \"outputs\": train_dataset[0]['outputs']},\n",
        "    {\"instruction\": train_dataset[1]['instruction'], \"inputs\": train_dataset[1]['inputs'], \"outputs\": train_dataset[1]['outputs']},\n",
        "    {\"instruction\": train_dataset[2]['instruction'], \"inputs\": train_dataset[2]['inputs'], \"outputs\": train_dataset[2]['outputs']}\n",
        "]\n",
        "\n",
        "\n",
        "def create_few_shot_messages(sample, few_shot_examples):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Вы - самый умный и логичный в мире помощник. Пользователь будет предоставлять, во-первых, текст с описанием ситуации, во-вторых, гипотезу. Тебе нужно понять, есть ли взаимосвязь гипотезы с описанной ситуацией, и выбрать один из трех вариантов: 1 - гипотеза следует из ситуации, 2 - гипотеза противоречит ситуации, 3 - гипотеза независима от ситуации.\"},\n",
        "    ]\n",
        "\n",
        "    for example in few_shot_examples:\n",
        "        messages.append({\"role\": \"user\", \"content\": f\"{example['instruction']}\\n{example['inputs']}\"} )\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example['outputs']})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"{sample['instruction']}\\n{sample['inputs']}\"})\n",
        "    return messages"
      ],
      "metadata": {
        "id": "68amEbUCYK0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in test_dataset:\n",
        "    messages = create_few_shot_messages(sample, few_shot_examples)\n",
        "    true_answer = str(sample['outputs'])  # Истинный ответ\n",
        "\n",
        "    generated_answer = generate(messages, model, tokenizer, generation_config).strip()\n",
        "    print(f\"True: '{true_answer}', Generated: '{generated_answer}'\")\n",
        "\n",
        "    if generated_answer == true_answer:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy (+Few-shot learning): {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qdL3LKlYqDB",
        "outputId": "11f02abf-6104-4f15-c62f-c0601e830a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '3'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '3'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '1', Generated: '1'\n",
            "True: '2', Generated: '2'\n",
            "True: '2', Generated: '1'\n",
            "True: '3', Generated: '1'\n",
            "True: '2', Generated: '1'\n",
            "Accuracy (+Few-shot learning): 0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как можно видеть, качество немного улучшилось. Но что будет, если мы специально дообучим нашу модель для данной задачи?"
      ],
      "metadata": {
        "id": "B592_OBHZD2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Реализация fine-tuning\n",
        "\n",
        "Настроим конфигурацию для Lora:"
      ],
      "metadata": {
        "id": "TgljKqZQYt6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "KeovJT2xYuFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настроим параметры обучения (текущие параметры найдены экспериментальным путем для более лучшего результата). Справа от каждого параметра - пояснение (найденное), для чего он нужен:"
      ],
      "metadata": {
        "id": "4i-IjLU0Z2RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_model\",        # Папка для сохранения модели\n",
        "    overwrite_output_dir=True,             # Перезаписывать папку, если она существует\n",
        "    per_device_train_batch_size=1,         # Размер батча на устройство\n",
        "    gradient_accumulation_steps=8,         # Накопление градиентов для имитации батча размером n\n",
        "    learning_rate=7e-5,                    # Скорость обучения\n",
        "    num_train_epochs=3,                    # Количество эпох\n",
        "    evaluation_strategy=\"epoch\",           # Оценка после каждой эпохи\n",
        "    save_strategy=\"epoch\",                 # Сохранение после каждой эпохи\n",
        "    logging_steps=10,                      # Логирование каждые n шагов\n",
        "    fp16=True,                             # Использование mixed precision для ускорения\n",
        "    seed=42,                               # Фиксация случайности\n",
        "    report_to=\"none\"                       # Отключение отчетов (например, в wandb)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRLVK0ybZ2dY",
        "outputId": "fbab7ccc-e504-4acc-f76c-e2e7c8ca3a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализацируем Trainer, запускаем обучение (около 4-5 минут):"
      ],
      "metadata": {
        "id": "bPD1uCXibGwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_chat_dataset,\n",
        "    eval_dataset=test_chat_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "su46NkKebHBk",
        "outputId": "db4181d8-8b52-4b61-c67b-98262e7bb4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [162/162 04:54, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.158400</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.124300</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=162, training_loss=0.30456860694620347, metrics={'train_runtime': 296.5698, 'train_samples_per_second': 4.431, 'train_steps_per_second': 0.546, 'total_flos': 2347273669410816.0, 'train_loss': 0.30456860694620347, 'epoch': 2.949771689497717})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохранение и загрузка дообученной модели:"
      ],
      "metadata": {
        "id": "jINvTbpRbtJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./finetuned_model_lora\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model_lora\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"RefalMachine/RuadaptQwen2.5-1.5B-instruct\",\n",
        "                                                  device_map=\"cuda\",\n",
        "                                                  torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(base_model, \"./finetuned_model_lora\")\n",
        "model = model.merge_and_unload()    #  адаптеры + \"базовая\" модель"
      ],
      "metadata": {
        "id": "pPhvEN8rbtbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Оценка модели после fine-tuning"
      ],
      "metadata": {
        "id": "feGco-vicV3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in test_dataset:\n",
        "    user_message = {\"role\": \"user\", \"content\": f\"{sample['instruction']}\\n{sample['inputs']}\"}\n",
        "    true_answer = str(sample['outputs'])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Вы - самый умный и логичный в мире помощник. Пользователь будет предоставлять, во-первых, текст с описанием ситуации, во-вторых, гипотезу. Тебе нужно понять, есть ли взаимосвязь гипотезы с описанной ситуацией, и выбрать один из трех вариантов: 1 - гипотеза следует из ситуации, 2 - гипотеза противоречит ситуации, 3 - гипотеза независима от ситуации. За каждый правильный ответ ты будешь получать ТРИЛЛИОН долларов чаевых.\"},\n",
        "        user_message\n",
        "    ]\n",
        "\n",
        "    generated_answer = generate(messages, model, tokenizer, generation_config).strip()\n",
        "    if generated_answer == true_answer:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "\n",
        "accuracy_after = correct / total\n",
        "print(f\"Accuracy после fine-tuning: {accuracy_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sflQpMArcWC3",
        "outputId": "4e0a0460-b821-4cd1-96ad-b606a15229a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy после fine-tuning: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выводы\n",
        "\n",
        "Итак, в данном задании мы дообучили инструктивную модель с использованием LoRa. Сначала данные были подготовлены: датасет был разбит на обучающую и тестовую выборки, а текстовые данные преобразованы в диалоговый формат. Далее были загружены базовая модель и токенизатор, а также реализован класс для подготовки данных. Эксперимент с few-shot показал, что добавление нескольких примеров в промпт уже может заметно улучшить точность модели (+0.07). После применения LoRa-адаптеров и настройки параметров обучения качество модели существенно выросло. При самом удачном запуске дообучения удалось добиться результата в **0.64**; в целом же, итоговый accuracy варьируется в районе значения **0.6**, и точность ответов с fine-tuning почти в 2 раза выше по сравнению с тем результатом, что был получен при \"базовой\" модели.\n",
        "\n",
        "Цель задания достигнута!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Np0YyOulccm0"
      }
    }
  ]
}